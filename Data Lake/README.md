
# Data Lake with Spark

## **Overview**

The objective of this project is provide a ETL pipeline using pyspark that extract the data from the Milling dataset 
and insert this information on the S3 on parquet format for his later analysis.


## **Song Dataset**
Songs dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/).

Sample Record :
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## **Log Dataset**
Logs dataset is generated by [Event Simulator](https://github.com/Interana/eventsim).

Sample Record :
```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```


## Schema

#### Fact Table 

![Image of production tables](production.png)

**songplays** - records in log data associated with song plays i.e. records with page `NextSong`

```
songplay_id, start_time, user_id, level, song_id, session_id, location, user_agent
```

#### Dimension Tables
**users**  - users in the app
```
user_id, first_name, last_name, gender, level
```
**songs**  - songs in music database
```
song_id, title, artist_id, year, duration
```
**artists**  - artists in music database
```
artist_id, name, location, latitude, longitude
```
**time**  - timestamps of records in  **songplays**  broken down into specific units
```
start_time, hour, day, week, month, year, weekday
```

## Project Files

- ```dl.cfg``` -> contains the configuration information to be able to connect and sync the information. 
This is not present in the repository
 
```
[default]
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
```

- ```etl.py``` -> Process **song_data** and **log_data**, input this information to the fact and dimension parquet files.
- ```tests.ipynb``` -> Is a notebook that have some testing code to check the functionality and quick test for development.


## Environment 
Python 3.7 or above

Spark 2.4

## How to run

Run the follow commands as below, to submit the project with Spark
```
export AWS_ACCESS_KEY_ID=
export AWS_SECRET_ACCESS_KEY=
spark-submit --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.7  etl.py 
``` 

#### Reference: 

[PySpark](https://spark.apache.org/docs/latest/api/python/index.html)

